---
id: 262
title: Facebook was right to run its test
date: 2014-07-01T18:10:54+00:00
author: tvladeck
layout: post
guid: http://tomvladeck.com/?p=262
permalink: /2014/07/01/facebook-was-right-to-run-its-test/
dsq_thread_id:
  - "2810174707"
categories:
  - Product
  - Tech
tags:
  - facebook
---
Lately, there has been <a href="http://mobile.nytimes.com/2014/06/30/technology/facebook-tinkers-with-users-emotions-in-news-feed-experiment-stirring-outcry.html?partner=rss&amp;emc=rss&amp;smid=tw-nytimes&amp;_r=0&amp;referrer=" target="_blank"><span style="color: #042eee;"><span style="text-decoration: underline;">an uproar</span></span></a> over <a href="http://www.pnas.org/content/111/24/8788.full.pdf" target="_blank"><span style="color: #042eee;"><span style="text-decoration: underline;">Facebook’s study on emotional contagion in a large number of users’ feeds</span></span></a>. The study seemed to indicate that Facebook had the ability to make people happier or sadder - that is, manipulate their emotions - by manipulating the content of their feeds.

Lots of people don’t like the idea of being unwitting lab rats for experiments, or at the prospect of a corporation having the ability to control our thoughts and emotions, or just the danger of conducting such a study:

https://twitter.com/laurenweinstein/status/483051171255312384

<span style="line-height: 1.6;">In my mind, however, these arguments rest on a faulty assumption, which is that Facebook isn’t already manipulating our emotions, at least by accident. Indeed, the studies done on this topic seem to suggest the opposite; Facebook already impacts its users’ emotions significantly. (</span><a href="http://www.huffingtonpost.com/2013/01/22/facebook-study-envy_n_2526549.html" target="_blank"><span style="color: #042eee;"><span style="text-decoration: underline;">one</span></span></a><span style="line-height: 1.6;">, </span><a href="http://www.economist.com/news/science-and-technology/21583593-using-social-network-seems-make-people-more-miserable-get-life" target="_blank"><span style="color: #042eee;"><span style="text-decoration: underline;">two</span></span></a><span style="line-height: 1.6;">)</span>

In addition, Facebook has to use some algorithm to show content to its users. I think it’s a mistake to assume that there is a “neutral” scenario, especially when all evidence is to the contrary.

https://twitter.com/fmanjoo/status/483274496598482945

Then this issue really becomes a question of understanding what’s <b>already happening</b>, not about doing something different qualitatively different, and it’s clear to me that in this frame Facebook should be conducting experiments like this (within limits, obviously).

The take by some, though, is that it’s not the experiment itself that’s troubling, but that it was conducted without its users knowing:

https://twitter.com/BenedictEvans/status/483105929433341952

But again, if there really is no neutral scenario, and your emotions are going to be manipulated in any sort of consumption of Facebook, then even before any intentional tests have been done, you’ve already opted in to having your emotions manipulated. Indeed, you opted in when you started using Facebook, and every time you added something to your Facebook universe, via a like or added friend, you increased the scope of what could impact you.

So I’m glad Facebook is doing this research, and I’m really glad they’re publishing it for external consumption.